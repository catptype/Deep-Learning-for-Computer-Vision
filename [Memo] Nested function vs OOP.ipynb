{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested function vs Object Oriented Programming (OOP): different code implementation make different result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This .ipynb is a memo that demonstrates how code programming implementations can impact the creation of deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"image_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"num_head\": 2,\n",
    "    \"latent_size\": 768,\n",
    "    \"trans_layer\": 1,\n",
    "    \"num_class\": 2,\n",
    "    \"mlp_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 1: Inner function or Nested function style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    LayerNormalization,\n",
    "    MultiHeadAttention,\n",
    "    Add,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "def ViT_Transformer():\n",
    "    \n",
    "    def PatchEncoder(input):\n",
    "        # Patching image\n",
    "        patch = tf.image.extract_patches(images=input, \n",
    "                                        sizes=[1,CFG[\"patch_size\"],CFG[\"patch_size\"],1],\n",
    "                                        strides=[1,CFG[\"patch_size\"],CFG[\"patch_size\"],1],\n",
    "                                        rates=[1,1,1,1],\n",
    "                                        padding=\"VALID\")\n",
    "\n",
    "        num_patch = patch.shape[1] * patch.shape[2]\n",
    "        new_shape = (-1, num_patch, patch.shape[-1])\n",
    "        patch = tf.reshape(patch, new_shape)\n",
    "        \n",
    "        # Linear projection and Positional embedding\n",
    "        embedding_input = tf.range(start=0, limit=num_patch, delta=1)\n",
    "        output = Dense(CFG[\"latent_size\"])(patch) + Embedding(num_patch, CFG[\"latent_size\"])(embedding_input)\n",
    "        return output\n",
    "    \n",
    "    def TransformerEncoder(input):\n",
    "        x1 = LayerNormalization()(input)\n",
    "        x1 = MultiHeadAttention(CFG[\"num_head\"], CFG[\"latent_size\"])(x1,x1)\n",
    "\n",
    "        x1 = Add()([x1, input])\n",
    "        \n",
    "        x2 = LayerNormalization()(x1)\n",
    "        x2 = Dense(CFG[\"latent_size\"], activation=\"gelu\")(x2)\n",
    "        x2 = Dense(CFG[\"latent_size\"], activation=\"gelu\")(x2)\n",
    "\n",
    "        output = Add()([x1, x2])\n",
    "        return output\n",
    "    \n",
    "    # Input image\n",
    "    input = Input(shape=(CFG[\"image_size\"], CFG[\"image_size\"], 3), name=\"Input_image\")\n",
    "\n",
    "    # Image patch encoder\n",
    "    x = PatchEncoder(input)\n",
    "\n",
    "    # Transformer encoder for x layers\n",
    "    for _ in range(CFG[\"trans_layer\"]):\n",
    "        x = TransformerEncoder(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(CFG[\"mlp_size\"], activation=\"gelu\")(x)\n",
    "    x = Dense(CFG[\"mlp_size\"], activation=\"gelu\")(x)\n",
    "    output = Dense(CFG[\"num_class\"], activation=\"softmax\", dtype=tf.float32)(x)\n",
    "\n",
    "    model = Model(inputs=[input], outputs=output, name=\"VIT_MODEL\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code1 = ViT_Transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 2: Object Oriented Programming (OOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Add,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "import tensorflow as tf\n",
    "from model.DeepLearningModel import DeepLearningModel\n",
    "  \n",
    "class PatchEncoder(Layer):\n",
    "    \"\"\"\n",
    "    PatchEncoder layer: Encodes image patches and applies linear projection with positional embedding.\n",
    "\n",
    "    Args:\n",
    "        patch_size (int): Size of the image patch.\n",
    "        num_patch (int): Number of patches in the image.\n",
    "        latent_size (int): Size of the latent space.\n",
    "\n",
    "    Attributes:\n",
    "        patch_size (int): Size of the image patch.\n",
    "        num_patch (int): Number of patches in the image.\n",
    "        latent_size (int): Size of the latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size, num_patch, latent_size):\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patch = num_patch\n",
    "        self.latent_size = latent_size\n",
    "        super(PatchEncoder, self).__init__(name=\"Patch_Encoder\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Builds the PatchEncoder layer by creating necessary sub-layers.\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): Shape of the input tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        self.linear_projection = Dense(self.latent_size)\n",
    "        self.positional_embedding = Embedding(self.num_patch, self.latent_size)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"\n",
    "        Applies the PatchEncoder layer to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            input (tensor): Input tensor containing the image.\n",
    "\n",
    "        Returns:\n",
    "            output (tensor): Encoded tensor with patch embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        # Patching image\n",
    "        patch = tf.image.extract_patches(\n",
    "            images=input,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch = tf.reshape(patch, (-1, self.num_patch, patch.shape[-1]))\n",
    "\n",
    "        # Linear projection and Positional embedding\n",
    "        embedding_input = tf.range(start=0, limit=self.num_patch, delta=1)\n",
    "        output = self.linear_projection(patch) + self.positional_embedding(embedding_input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    \"\"\"\n",
    "    TransformerEncoder layer: Applies multi-head self-attention and feed-forward layers.\n",
    "\n",
    "    Args:\n",
    "        num_head (int): Number of attention heads.\n",
    "        latent_size (int): Size of the latent space.\n",
    "\n",
    "    Attributes:\n",
    "        num_head (int): Number of attention heads.\n",
    "        latent_size (int): Size of the latent space.\n",
    "    \"\"\"\n",
    "    num_instances = 0\n",
    "\n",
    "    def __init__(self, num_head, latent_size):\n",
    "        self.num_head = num_head\n",
    "        self.latent_size = latent_size\n",
    "        TransformerEncoder.num_instances += 1\n",
    "        layer_name = f\"Transformer_Encoder_{TransformerEncoder.num_instances}\"\n",
    "        super(TransformerEncoder, self).__init__(name=layer_name)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Builds the TransformerEncoder layer by creating necessary sub-layers.\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): Shape of the input tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "        self.multi_head = MultiHeadAttention(self.num_head, self.latent_size)\n",
    "        self.mlp1 = Dense(self.latent_size, activation=\"gelu\")\n",
    "        self.mlp2 = Dense(self.latent_size, activation=\"gelu\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"\n",
    "        Applies the TransformerEncoder layer to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            input (tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            output (tensor): Transformed tensor after multi-head self-attention and feed-forward layers.\n",
    "\n",
    "        \"\"\"\n",
    "        x1 = self.layer_norm1(input)\n",
    "        x1 = self.multi_head(x1, x1)\n",
    "        x1 = Add()([x1, input])\n",
    "\n",
    "        x2 = self.layer_norm2(x1)\n",
    "        x2 = self.mlp1(x2)\n",
    "        x2 = self.mlp2(x2)\n",
    "        output = Add()([x1, x2])\n",
    "        return output\n",
    "\n",
    "\n",
    "class ViTModel(DeepLearningModel):\n",
    "    \"\"\"\n",
    "    ViTModel: Vision Transformer model for image classification.\n",
    "\n",
    "    Args:\n",
    "        image_size (int): Size of the input image.\n",
    "        patch_size (int): Size of the image patch.\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "        num_head (int): Number of attention heads.\n",
    "        latent_size (int): Size of the latent space.\n",
    "        num_layer (int): Number of transformer layers.\n",
    "        mlp_size (int): Size of the multi-layer perceptron.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        num_classes,\n",
    "        num_head,\n",
    "        latent_size,\n",
    "        num_layer,\n",
    "        mlp_size,\n",
    "    ):\n",
    "        assert image_size % patch_size == 0, f\"Image size ({image_size}) is not divisible by Patch size ({patch_size})\"\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_head = num_head\n",
    "        self.latent_size = latent_size\n",
    "        self.num_layer = num_layer\n",
    "        self.mlp_size = mlp_size\n",
    "        super().__init__(image_size=image_size, num_classes=num_classes)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the ViTModel architecture.\n",
    "\n",
    "        Returns:\n",
    "            model (tensorflow.keras.Model): Compiled ViT model.\n",
    "\n",
    "        \"\"\"        \n",
    "        # Input layer\n",
    "        input = Input(shape=(self.image_size, self.image_size, 3), name=\"Input_image\")\n",
    "\n",
    "        # Image patch encoder\n",
    "        x = PatchEncoder(patch_size=self.patch_size,\n",
    "                         num_patch=(self.image_size // self.patch_size) ** 2,\n",
    "                         latent_size=self.latent_size)(input)\n",
    "\n",
    "        # Transformer encoder\n",
    "        for _ in range(self.num_layer):\n",
    "            x = TransformerEncoder(num_head=self.num_head, latent_size=self.latent_size)(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(self.mlp_size, activation=\"gelu\")(x)\n",
    "        x = Dense(self.mlp_size, activation=\"gelu\")(x)\n",
    "        output = Dense(self.num_classes, activation=\"softmax\", dtype=tf.float32)(x)\n",
    "\n",
    "        model_name = f\"ViT_L{self.num_layer}_I{self.image_size}x{self.image_size}_P{self.patch_size}_H{self.num_head}_D{self.latent_size}_MLP{self.mlp_size}_{self.num_classes}Class\"\n",
    "        model = Model(inputs=[input], outputs=output, name=model_name)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2 = ViTModel(image_size=CFG[\"image_size\"],\n",
    "                 patch_size=CFG[\"patch_size\"],\n",
    "                 num_classes=CFG[\"num_class\"],\n",
    "                 num_head=CFG[\"num_head\"],\n",
    "                 latent_size=CFG[\"latent_size\"],\n",
    "                 num_layer=CFG[\"trans_layer\"],\n",
    "                 mlp_size=CFG[\"mlp_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify class PatchEncoder from Code 2\n",
    "\n",
    "Add line `self.positional_embedding.trainable = False` in `build` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(Layer):\n",
    "    \"\"\"\n",
    "    PatchEncoder layer: Encodes image patches and applies linear projection with positional embedding.\n",
    "\n",
    "    Args:\n",
    "        patch_size (int): Size of the image patch.\n",
    "        num_patch (int): Number of patches in the image.\n",
    "        latent_size (int): Size of the latent space.\n",
    "\n",
    "    Attributes:\n",
    "        patch_size (int): Size of the image patch.\n",
    "        num_patch (int): Number of patches in the image.\n",
    "        latent_size (int): Size of the latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size, num_patch, latent_size):\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patch = num_patch\n",
    "        self.latent_size = latent_size\n",
    "        super(PatchEncoder, self).__init__(name=\"Patch_Encoder\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Builds the PatchEncoder layer by creating necessary sub-layers.\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): Shape of the input tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        self.linear_projection = Dense(self.latent_size)\n",
    "        self.positional_embedding = Embedding(self.num_patch, self.latent_size)\n",
    "        self.positional_embedding.trainable = False\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"\n",
    "        Applies the PatchEncoder layer to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            input (tensor): Input tensor containing the image.\n",
    "\n",
    "        Returns:\n",
    "            output (tensor): Encoded tensor with patch embeddings.\n",
    "\n",
    "        \"\"\"\n",
    "        # Patching image\n",
    "        patch = tf.image.extract_patches(\n",
    "            images=input,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch = tf.reshape(patch, (-1, self.num_patch, patch.shape[-1]))\n",
    "\n",
    "        # Linear projection and Positional embedding\n",
    "        embedding_input = tf.range(start=0, limit=self.num_patch, delta=1)\n",
    "        output = self.linear_projection(patch) + self.positional_embedding(embedding_input)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code3 = ViTModel(image_size=CFG[\"image_size\"],\n",
    "                 patch_size=CFG[\"patch_size\"],\n",
    "                 num_classes=CFG[\"num_class\"],\n",
    "                 num_head=CFG[\"num_head\"],\n",
    "                 latent_size=CFG[\"latent_size\"],\n",
    "                 num_layer=CFG[\"trans_layer\"],\n",
    "                 mlp_size=CFG[\"mlp_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision Code 1 vs Code 2 vs Code 3\n",
    "\n",
    "In comparison, the Object-Oriented Programming implementations (Code 2 & 3) have a higher number of trainable parameters compared to the nested function approach (Code 1), as the trainable parameters in the embedding layer of Code 1 were excluded.\n",
    "\n",
    "The proof is that when the 'PatchEncoder' class in Code 2 is modified, the number of trainable parameters in Code 3 becomes equal to Code 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VIT_MODEL\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_image (InputLayer)        [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.image.extract_patches (TFOpL (None, 14, 14, 768)  0           Input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (None, 196, 768)     0           tf.image.extract_patches[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 196, 768)     590592      tf.reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 196, 768)     0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 196, 768)     1536        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 196, 768)     4723968     layer_normalization[0][0]        \n",
      "                                                                 layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 196, 768)     0           multi_head_attention[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 196, 768)     1536        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 196, 768)     590592      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 196, 768)     590592      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 196, 768)     0           add[0][0]                        \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 150528)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          19267712    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          16512       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            258         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 25,783,298\n",
      "Trainable params: 25,783,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "code1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ViT_L1_I224x224_P16_H2_D768_MLP128_2Class\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_image (InputLayer)     [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Patch_Encoder (PatchEncoder) (None, 196, 768)          741120    \n",
      "_________________________________________________________________\n",
      "Transformer_Encoder_1 (Trans (None, 196, 768)          5908224   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 150528)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               19267712  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 25,933,826\n",
      "Trainable params: 25,933,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "code2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ViT_L1_I224x224_P16_H2_D768_MLP128_2Class\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input_image (InputLayer)     [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "Patch_Encoder (PatchEncoder) (None, 196, 768)          741120    \n",
      "_________________________________________________________________\n",
      "Transformer_Encoder_2 (Trans (None, 196, 768)          5908224   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 150528)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               19267712  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 25,933,826\n",
      "Trainable params: 25,783,298\n",
      "Non-trainable params: 150,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "code3.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
