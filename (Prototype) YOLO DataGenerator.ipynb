{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype YOLO Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid out of memory errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"dataset_dir\":  \"D:\\Github Public\\DATASET MANAGER\\ObjectDetection_SIMPLE\",\n",
    "    \"image_size\": (224, 224),\n",
    "    \"anchor\": 3,\n",
    "    \"batch\": 8,\n",
    "    \"annotation_dict\": {\"Face\": 0,\"Oppai\": 1}, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3Augmentation:\n",
    "    def __init__(self, image_size, translate_range, rotation_range):\n",
    "        self.__image_size = image_size\n",
    "        self.__translate_range = translate_range\n",
    "        self.__rotation_range = rotation_range\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def horizontal_flip(self, image, annotation_list):\n",
    "        # Flip image\n",
    "        image = cv2.flip(image, 1)  # Flip horizontally\n",
    "\n",
    "        # Update annotation list\n",
    "        updated_annotation_list = []\n",
    "\n",
    "        for annotation in annotation_list:\n",
    "            for class_idx, xmin, ymin, xmax, ymax in [annotation]:\n",
    "                # Invert the x-coordinates\n",
    "                xmin, xmax = 1 - xmax, 1 - xmin  \n",
    "                \n",
    "                # Create a new annotation tuple\n",
    "                new_annotation = (class_idx, xmin, ymin, xmax, ymax)\n",
    "                updated_annotation_list.append(new_annotation)\n",
    "        \n",
    "        return tf.convert_to_tensor(image), tf.convert_to_tensor(updated_annotation_list)\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def vertical_flip(self, image, annotation_list):\n",
    "        # Flip image\n",
    "        image = cv2.flip(image, 0)  # Flip vertically\n",
    "\n",
    "        # Update annotation list\n",
    "        updated_annotation_list = []\n",
    "\n",
    "        for annotation in annotation_list:\n",
    "            for class_idx, xmin, ymin, xmax, ymax in [annotation]:\n",
    "                # Invert the x-coordinates\n",
    "                ymin, ymax = 1 - ymax, 1 - ymin  \n",
    "                \n",
    "                # Create a new annotation tuple\n",
    "                new_annotation = (class_idx, xmin, ymin, xmax, ymax)\n",
    "                updated_annotation_list.append(new_annotation)\n",
    "        \n",
    "        return tf.convert_to_tensor(image), tf.convert_to_tensor(updated_annotation_list)\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def translation(self, image, annotation_list):\n",
    "\n",
    "        if isinstance(self.__translate_range, tuple) and len(self.__translate_range) == 2:\n",
    "            # translation (percentage) of image width & height as tuple\n",
    "            max_translation_x, max_translation_y = self.__translate_range\n",
    "        \n",
    "        elif isinstance(self.__translate_range, float):\n",
    "            # translation (percentage) of image width & height as float number\n",
    "            max_translation_x = self.__translate_range\n",
    "            max_translation_y = self.__translate_range\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid translation format\")\n",
    "        \n",
    "        image_height, image_width = image.shape[:2]\n",
    "        \n",
    "        # Calculate pixel translations based on normalized translations\n",
    "        translation_x_pixels = int(max_translation_x * image_width)\n",
    "        translation_y_pixels = int(max_translation_y * image_height)\n",
    "\n",
    "        # Generate random translations within the pixel limits\n",
    "        translation_x = np.random.randint(-translation_x_pixels, translation_x_pixels)\n",
    "        translation_y = np.random.randint(-translation_y_pixels, translation_y_pixels)\n",
    "\n",
    "        # Translate the image\n",
    "        M = np.array([\n",
    "            [1, 0, translation_x], # Translate image in horizontal direction\n",
    "            [0, 1, translation_y], # Translate image in vertical direction\n",
    "        ], dtype=np.float32)\n",
    "        image = cv2.warpAffine(image, M, (image_width, image_height))\n",
    "\n",
    "        # Update annotation list\n",
    "        updated_annotation_list = []\n",
    "\n",
    "        for annotation in annotation_list:\n",
    "            for class_idx, xmin, ymin, xmax, ymax in [annotation]:\n",
    "\n",
    "                # Translate the bounding box coordinates\n",
    "                xmin += translation_x / image_width\n",
    "                xmax += translation_x / image_width\n",
    "                ymin += translation_y / image_height\n",
    "                ymax += translation_y / image_height\n",
    "\n",
    "                # Checking position\n",
    "                xmin = max(0, xmin)\n",
    "                xmax = min(1, xmax)\n",
    "                ymin = max(0, ymin)\n",
    "                ymax = min(1, ymax)\n",
    "                \n",
    "                # Create a new annotation tuple\n",
    "                new_annotation = (class_idx, xmin, ymin, xmax, ymax)\n",
    "                updated_annotation_list.append(new_annotation)\n",
    "\n",
    "        return tf.convert_to_tensor(image), tf.convert_to_tensor(updated_annotation_list)\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def rotation_complex(self, image, annotation_list): # more computation and more accurate when high rotation\n",
    "        # Define maximum allowed rotation angle in degrees\n",
    "        max_rotation_angle = self.__rotation_range  \n",
    "\n",
    "        image_height, image_width = image.shape[:2]\n",
    "        rotation_angle = np.random.randint(-max_rotation_angle, max_rotation_angle)\n",
    "\n",
    "        # Rotate the image\n",
    "        M = cv2.getRotationMatrix2D((image_width / 2, image_height / 2), rotation_angle, 1)\n",
    "        image = cv2.warpAffine(image, M, (image_width, image_height))\n",
    "\n",
    "        # Update annotation list\n",
    "        updated_annotation_list = []\n",
    "\n",
    "        for annotation in annotation_list:\n",
    "            for class_idx, xmin, ymin, xmax, ymax in [annotation]:\n",
    "\n",
    "                # Convert normalized bounding box coordinates to pixel coordinates\n",
    "                xmin *= image_width\n",
    "                ymin *= image_height\n",
    "                xmax *= image_width\n",
    "                ymax *= image_height\n",
    "\n",
    "                # Create an array of points representing the corners of the bounding box\n",
    "                points = np.array([(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)], dtype=np.float32)\n",
    "\n",
    "                # Add a row of ones to the points array\n",
    "                points = np.column_stack((points, np.ones(4, dtype=np.float32)))\n",
    "\n",
    "                # Rotate the points using matrix multiplication with M\n",
    "                rotated_points = (np.dot(M, points.T).T).astype(np.float32)\n",
    "\n",
    "                # Find the rotated bounding rectangle\n",
    "                rotated_rect = cv2.minAreaRect(rotated_points[:, :2])\n",
    "\n",
    "                # Get the rotated bounding box vertices\n",
    "                rotated_box = cv2.boxPoints(rotated_rect).astype(int)\n",
    "\n",
    "                # Find the new coordinates of the rotated bounding box\n",
    "                rotated_xmin = min(rotated_box[:, 0])\n",
    "                rotated_ymin = min(rotated_box[:, 1])\n",
    "                rotated_xmax = max(rotated_box[:, 0])\n",
    "                rotated_ymax = max(rotated_box[:, 1])\n",
    "\n",
    "                # Convert rotated coordinates back to normalized coordinates\n",
    "                rotated_xmin /= image_width\n",
    "                rotated_ymin /= image_height\n",
    "                rotated_xmax /= image_width\n",
    "                rotated_ymax /= image_height\n",
    "\n",
    "                # Create a new annotation tuple\n",
    "                new_annotation = (class_idx, rotated_xmin, rotated_ymin, rotated_xmax, rotated_ymax)\n",
    "                updated_annotation_list.append(new_annotation)\n",
    "\n",
    "        return tf.convert_to_tensor(image), tf.convert_to_tensor(updated_annotation_list)\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def rotation(self, image, annotation_list): # not accurate when high rotation\n",
    "        # Define maximum allowed rotation angle in degrees\n",
    "        max_rotation_angle = self.__rotation_range  \n",
    "\n",
    "        image_height, image_width = image.shape[:2]\n",
    "        rotation_angle = np.random.randint(-max_rotation_angle, max_rotation_angle)\n",
    "\n",
    "        # Rotate the image\n",
    "        M = cv2.getRotationMatrix2D((image_width / 2, image_height / 2), rotation_angle, 1)\n",
    "        image = cv2.warpAffine(image, M, (image_width, image_height))\n",
    "\n",
    "        # Update annotation list\n",
    "        updated_annotation_list = []\n",
    "\n",
    "        for annotation in annotation_list:\n",
    "            for class_idx, xmin, ymin, xmax, ymax in [annotation]:\n",
    "\n",
    "                # Convert normalized bounding box coordinates to pixel coordinates\n",
    "                xmin *= image_width\n",
    "                ymin *= image_height\n",
    "                xmax *= image_width\n",
    "                ymax *= image_height\n",
    "\n",
    "                # Create an array of points representing the corners of the bounding box\n",
    "                points = np.array([(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)])\n",
    "\n",
    "                # Add a column of ones to the points array\n",
    "                points_homogeneous = np.column_stack((points, np.ones(4)))\n",
    "\n",
    "                # Rotate the points using matrix multiplication with M\n",
    "                rotated_points = np.dot(M, points_homogeneous.T).T\n",
    "\n",
    "                # Find the new coordinates of the rotated bounding box\n",
    "                rotated_xmin = min(rotated_points[:, 0])\n",
    "                rotated_ymin = min(rotated_points[:, 1])\n",
    "                rotated_xmax = max(rotated_points[:, 0])\n",
    "                rotated_ymax = max(rotated_points[:, 1])\n",
    "\n",
    "                # Convert rotated coordinates back to normalized coordinates\n",
    "                rotated_xmin /= image_width\n",
    "                rotated_ymin /= image_height\n",
    "                rotated_xmax /= image_width\n",
    "                rotated_ymax /= image_height\n",
    "\n",
    "                # Create a new annotation tuple\n",
    "                new_annotation = (class_idx, rotated_xmin, rotated_ymin, rotated_xmax, rotated_ymax)\n",
    "                updated_annotation_list.append(new_annotation)\n",
    "\n",
    "        return tf.convert_to_tensor(image), tf.convert_to_tensor(updated_annotation_list)\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def padding(self, image, annotation_list):\n",
    "        \n",
    "        image_height, image_width = image.shape[:2] # OpenCV format\n",
    "        target_width, target_height = self.__image_size\n",
    "\n",
    "        # Calculate the padding sizes for both width and height\n",
    "        pad_width = max(0, target_width - image_width)\n",
    "        pad_height = max(0, target_height - image_height)\n",
    "\n",
    "        # Calculate the padding amounts for top, bottom, left, and right\n",
    "        top_pad = pad_height // 2\n",
    "        bottom_pad = pad_height - top_pad\n",
    "        left_pad = pad_width // 2\n",
    "        right_pad = pad_width - left_pad\n",
    "\n",
    "        # Pad the image with zeros (black)\n",
    "        image = cv2.copyMakeBorder(image, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=0)\n",
    "\n",
    "        # Update annotation list\n",
    "        updated_annotation_list = []\n",
    "\n",
    "        for annotation in annotation_list:\n",
    "            for class_idx, xmin, ymin, xmax, ymax in [annotation]:\n",
    "                # Adjust the bounding box coordinates for padding and normalize them\n",
    "                image_height_padded, image_width_padded = image.shape[:2]\n",
    "                xmin = (xmin * image_width + left_pad) / image_width_padded\n",
    "                ymin = (ymin * image_height + top_pad) / image_height_padded\n",
    "                xmax = (xmax * image_width + left_pad) / image_width_padded\n",
    "                ymax = (ymax * image_height + top_pad) / image_height_padded\n",
    "\n",
    "                # Create a new annotation tuple\n",
    "                new_annotation = (class_idx, xmin, ymin, xmax, ymax)\n",
    "                updated_annotation_list.append(new_annotation)\n",
    "\n",
    "        return tf.convert_to_tensor(image), tf.convert_to_tensor(updated_annotation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3DataGenerator:\n",
    "    def __init__(self, \n",
    "                 input,\n",
    "                 annotation_dict,\n",
    "                 image_size, \n",
    "                 num_anchor, \n",
    "                 horizontal_flip=False, \n",
    "                 vertical_flip=False, \n",
    "                 translate_range=None, \n",
    "                 rotation_range=None,\n",
    "                 ):\n",
    "        \n",
    "        # Check input format\n",
    "        if not self.__is_valid_input(input):\n",
    "            raise ValueError(\"Invalid input format\")\n",
    "        \n",
    "        # Check image_size format\n",
    "        if not (isinstance(image_size, tuple) and len(image_size) == 2):\n",
    "            raise ValueError(\"Invalid image_size. It should be a tuple (width, height).\")\n",
    "        \n",
    "        # Check translation format (tuple or float)\n",
    "        if not self.__is_valid_translation(translate_range):\n",
    "            raise ValueError(\"Invalid translation format. It should be a tuple (x, y) or a float.\")\n",
    "\n",
    "        # Private artibute    \n",
    "        self.__input = input\n",
    "        self.__annotation_dict = annotation_dict\n",
    "        self.__image_size = image_size\n",
    "        self.__num_anchor = num_anchor\n",
    "        self.__horizontal_flip = horizontal_flip\n",
    "        self.__vertical_flip = vertical_flip\n",
    "        self.__translate_range = translate_range\n",
    "        self.__rotation_range = rotation_range\n",
    "\n",
    "        # Public artribute\n",
    "        self.dataset = None\n",
    "        \n",
    "    # Private methods\n",
    "    def __is_valid_input(self, input):\n",
    "        if isinstance(input, list):\n",
    "            checker = [isinstance(item, tuple) and len(item) == 2 for item in input]\n",
    "        return all(checker)\n",
    "    \n",
    "    def __is_valid_translation(self, translate_range):\n",
    "        is_valid_tuple = isinstance(translate_range, tuple) and len(translate_range) == 2\n",
    "        is_valid_float = isinstance(translate_range, float)\n",
    "        is_none = translate_range is None\n",
    "        return is_valid_tuple or is_valid_float or is_none\n",
    "\n",
    "    def __find_best_anchor(self, width, height, anchor_list):\n",
    "        best_iou = 0\n",
    "        best_anchor = None\n",
    "\n",
    "        for idx, anchor in enumerate(anchor_list):\n",
    "            anchor_width, anchor_height = anchor\n",
    "            intersection = min(width, anchor_width) * min(height, anchor_height)\n",
    "            union = width * height + anchor_width * anchor_height - intersection\n",
    "            iou = intersection / union\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_anchor = idx\n",
    "\n",
    "        return best_anchor\n",
    "\n",
    "    def __calculate_anchor(self, annotation_list):\n",
    "\n",
    "        # Calculate n_anchor boudary box sizes (width, height) from all annotations\n",
    "        bbox_size_list = []\n",
    "        \n",
    "        for annotation in annotation_list:\n",
    "            for _, xmin, ymin, xmax, ymax in annotation:\n",
    "                width = xmax - xmin\n",
    "                height = ymax - ymin\n",
    "                bbox_size_list.append([width, height])\n",
    "        \n",
    "        bbox_size_list = np.array(bbox_size_list)\n",
    "\n",
    "        # Perform k-means clustering\n",
    "        kmeans = KMeans(n_clusters=self.__num_anchor, random_state=0)\n",
    "        kmeans.fit(bbox_size_list)\n",
    "\n",
    "        # Get the cluster centroids, which represent the anchor box sizes\n",
    "        anchor_boxes = kmeans.cluster_centers_\n",
    "\n",
    "        # Convert the NumPy array to a list of tuples\n",
    "        anchor_boxes = [tuple(map(lambda x: round(x, 2), row)) for row in anchor_boxes]\n",
    "\n",
    "        return anchor_boxes\n",
    "\n",
    "    def __image_reader_cv(self, image_path):\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image from {image_path}\")\n",
    "    \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get the original image dimensions\n",
    "        original_height, original_width, _ = image.shape\n",
    "\n",
    "        # Calculate the aspect ratio\n",
    "        aspect_ratio = original_width / original_height\n",
    "\n",
    "        # Calculate the new dimensions while maintaining the aspect ratio\n",
    "        target_width, target_height = self.__image_size\n",
    "        \n",
    "        if aspect_ratio == 1:\n",
    "            # Perfect square shape\n",
    "            new_width = min(self.__image_size)\n",
    "            new_height = min(self.__image_size)   \n",
    "        elif aspect_ratio > 1:\n",
    "            # Landscape orientation (wider than tall)\n",
    "            new_width = target_width\n",
    "            new_height = int(new_width / aspect_ratio)\n",
    "        else:\n",
    "            # Portrait or square orientation (taller than wide)\n",
    "            new_height = target_height\n",
    "            new_width = int(new_height * aspect_ratio)\n",
    "        \n",
    "        # Resize the image using interpolation\n",
    "        image = cv2.resize(image, (new_width, new_height), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Convert pixel values from int to float by dividing by 255.0\n",
    "        image = image / 255.0\n",
    "        return image.astype(\"float32\")\n",
    "    \n",
    "    def __image_reader_tf(self, image_path):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_image(image, expand_animations=False)\n",
    "        image = tf.image.resize(image, self.__image_size,\n",
    "                                preserve_aspect_ratio=True,\n",
    "                                method=\"bicubic\",\n",
    "                                antialias=True)\n",
    "        image = image / 255.0\n",
    "        return image\n",
    "    \n",
    "    def __xml_reader(self, xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        annotation_list = []\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            # Class name or label\n",
    "            class_name = obj.find('classname').text\n",
    "            class_idx = self.__annotation_dict[class_name]\n",
    "            \n",
    "            # Boundary box positions\n",
    "            xmin = float(obj.find('bndbox/xmin').text)\n",
    "            ymin = float(obj.find('bndbox/ymin').text)\n",
    "            xmax = float(obj.find('bndbox/xmax').text)\n",
    "            ymax = float(obj.find('bndbox/ymax').text)\n",
    "\n",
    "            annotation = (class_idx, xmin, ymin, xmax, ymax)\n",
    "\n",
    "            annotation_list.append(annotation)\n",
    "        \n",
    "        return tf.convert_to_tensor(annotation_list)\n",
    "    \n",
    "    def __generate_annotation_label(self, annotation_list):\n",
    "\n",
    "        label_data = []\n",
    "\n",
    "        for downsampling_scale in [32, 16, 8]: # YOLOv3 always produce 3 scales prediction\n",
    "            # Calculate grid size for the current scale\n",
    "            x_grid_size = self.__image_size[0] // downsampling_scale\n",
    "            y_grid_size = self.__image_size[1] // downsampling_scale\n",
    "\n",
    "            # Initialize label data for the current scale\n",
    "            label_data_shape = (x_grid_size, y_grid_size, self.__num_anchor, 5 + len(self.__annotation_dict))\n",
    "            scale_label_data = np.zeros(label_data_shape, dtype=np.float32)\n",
    "            \n",
    "            for annotation in annotation_list:\n",
    "                for class_id, xmin, ymin, xmax, ymax in [annotation]:\n",
    "                \n",
    "                    # Calculate object center, width, and height\n",
    "                    x_center = (xmin + xmax) / 2.0\n",
    "                    y_center = (ymin + ymax) / 2.0\n",
    "                    width = xmax - xmin\n",
    "                    height = ymax - ymin\n",
    "\n",
    "                    # Find the best anchor for the current object based on its size\n",
    "                    best_anchor = self.__find_best_anchor(width, height, self.__anchor_boxes)\n",
    "\n",
    "                    # Convert box coordinates and size to grid cell coordinates\n",
    "                    x_grid_pos, y_grid_pos = int(x_center * x_grid_size), int(y_center * y_grid_size)\n",
    "\n",
    "                    # Encode object information into the label_data tensor for the current scale\n",
    "                    scale_label_data[x_grid_pos, y_grid_pos, best_anchor, 0] = 1.  # Objectness score\n",
    "                    scale_label_data[x_grid_pos, y_grid_pos, best_anchor, 1] = (x_center * x_grid_size) % 1\n",
    "                    scale_label_data[x_grid_pos, y_grid_pos, best_anchor, 2] = (y_center * y_grid_size) % 1\n",
    "                    scale_label_data[x_grid_pos, y_grid_pos, best_anchor, 3] = width\n",
    "                    scale_label_data[x_grid_pos, y_grid_pos, best_anchor, 4] = height\n",
    "                    scale_label_data[x_grid_pos, y_grid_pos, best_anchor, 5 + int(class_id)] = 1.  # Class one-hot encoding\n",
    "            \n",
    "                label_data.append(scale_label_data)\n",
    "\n",
    "        return label_data[0], label_data[1], label_data[2]\n",
    "    \n",
    "    #@tf.function\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def __preprocessing(self, image_path, xml_path):\n",
    "\n",
    "        print(\"CALLING preprocessing function\")\n",
    "        print(\"READ IMAGE\")\n",
    "        image = self.__image_reader_tf(image_path)\n",
    "        print(\"READ XML\")\n",
    "        annotation_list = tf.numpy_function(\n",
    "            func=self.__xml_reader,\n",
    "            inp=[xml_path],\n",
    "            Tout=tf.float32,\n",
    "            name=\"XML_reader\",\n",
    "        )\n",
    "        \n",
    "        augment = YOLOv3Augmentation(\n",
    "            image_size = self.__image_size, \n",
    "            translate_range = self.__translate_range, \n",
    "            rotation_range = self.__rotation_range,\n",
    "        )\n",
    "\n",
    "        print(\"AUGMENTATION: \", end=\"\")\n",
    "        if self.__translate_range is not None:\n",
    "            print(\"translation -> \", end=\"\")\n",
    "            image, annotation_list = tf.numpy_function(\n",
    "                func=augment.translation,\n",
    "                inp=[image, annotation_list],\n",
    "                Tout=[tf.float32, tf.float32],\n",
    "                name=\"Image_translation\",\n",
    "            )\n",
    "\n",
    "        if self.__rotation_range is not None:\n",
    "            print(\"rotation -> \", end=\"\")\n",
    "            image, annotation_list = tf.numpy_function(\n",
    "                func=augment.rotation_complex,\n",
    "                inp=[image, annotation_list],\n",
    "                Tout=[tf.float32, tf.float32],\n",
    "                name=\"Image_rotation\",\n",
    "            )\n",
    "\n",
    "        if self.__horizontal_flip and random.random() < 0.5:\n",
    "            print(\"horizontal flip -> \", end=\"\")\n",
    "            image, annotation_list = tf.numpy_function(\n",
    "                func=augment.horizontal_flip,\n",
    "                inp=[image, annotation_list],\n",
    "                Tout=[tf.float32, tf.float32],\n",
    "                name=\"Image_horizontal_flip\",\n",
    "            )\n",
    "\n",
    "        if self.__vertical_flip and random.random() < 0.5:\n",
    "            print(\"vertical flip -> \", end=\"\")\n",
    "            image, annotation_list = tf.numpy_function(\n",
    "                func=augment.vertical_flip,\n",
    "                inp=[image, annotation_list],\n",
    "                Tout=[tf.float32, tf.float32],\n",
    "                name=\"Image_vertical_flip\",\n",
    "            )\n",
    "        \n",
    "        print(\"padding ... \", end=\"\")\n",
    "        image, annotation_list = tf.numpy_function(\n",
    "            func=augment.padding,\n",
    "            inp=[image, annotation_list],\n",
    "            Tout=[tf.float32, tf.float32],\n",
    "            name=\"Image_padding\",\n",
    "        )\n",
    "        print(\"COMPLETE\")\n",
    "\n",
    "        print(\"generate annotation label ... \", end=\"\")\n",
    "        small, medium, large = tf.numpy_function(\n",
    "            func=self.__generate_annotation_label,\n",
    "            inp=[annotation_list],\n",
    "            Tout=[tf.float32, tf.float32, tf.float32],\n",
    "            name=\"Annotation_labeling\",\n",
    "        )\n",
    "        print(\"COMPLETE\")\n",
    "\n",
    "        return tf.convert_to_tensor(image), (small, medium, large)\n",
    "\n",
    "    # Public methods\n",
    "    def generate_dataset(self, batch_size, drop_reminder=False):\n",
    "        \n",
    "        # Check batch_size\n",
    "        if not isinstance(batch_size, int):\n",
    "            raise ValueError(\"Invalid batch_size. It should be an integer.\")\n",
    "\n",
    "        # file path\n",
    "        image_path = [image for image, _ in self.__input]\n",
    "        xml_path = [xml for _, xml in self.__input]\n",
    "\n",
    "        # calculate anchor boxes\n",
    "        print(\"Calculating anchor size ... \", end=\"\")\n",
    "        annotation_list = [self.__xml_reader(path) for path in xml_path]\n",
    "        self.__anchor_boxes = self.__calculate_anchor(annotation_list)\n",
    "        print(f\"{self.__num_anchor} anchor box{'es' if self.__num_anchor > 1 else ''}: {self.__anchor_boxes}\")\n",
    "\n",
    "        #print(\"Prepare raw dataset ...\", end=\"\")\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_path, xml_path))       \n",
    "        dataset = dataset.map(self.__preprocessing)\n",
    "        dataset = dataset.shuffle(buffer_size=len(dataset))\n",
    "        dataset = dataset.batch(batch_size=batch_size, drop_remainder=drop_reminder)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        self.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROGRAMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [os.path.join(root,image) for root, _, files in os.walk(CFG[\"dataset_dir\"], topdown=True) for image in files]\n",
    "path_list = [path for path in path_list if \"_ROI\" not in path]\n",
    "path_list = [path for path in path_list if \".csv\" not in path]\n",
    "path_list = [(path_list[i], path_list[i+1]) for i in range (0,len(path_list),2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = YOLOv3DataGenerator(\n",
    "    input=path_list,\n",
    "    annotation_dict=CFG[\"annotation_dict\"],\n",
    "    image_size=CFG[\"image_size\"], \n",
    "    num_anchor=CFG[\"anchor\"], \n",
    "    horizontal_flip=False, \n",
    "    vertical_flip=False, \n",
    "    translate_range=None, \n",
    "    rotation_range=20,\n",
    ")\n",
    "\n",
    "data_generator.generate_dataset(\n",
    "    batch_size=CFG[\"batch\"],\n",
    "    drop_reminder=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {}\n",
    "\n",
    "for key, value in CFG[\"annotation_dict\"].items():\n",
    "    class_map[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_batch(dataset, class_mapping, figsize=(10,10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for images, labels in dataset.take(1):\n",
    "        # Calculate number of picture\n",
    "        base = 2\n",
    "        while True:\n",
    "            if math.log(len(images), base) <= 2:\n",
    "                break   \n",
    "            base += 1\n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            plt.subplot(base, base, batch_idx + 1)\n",
    "            image = (images[batch_idx] * 255).numpy().astype(\"uint8\")\n",
    "            image_with_boxes = image.copy()\n",
    "            title = \"\"\n",
    "\n",
    "            for scale_idx, scale_labels in enumerate(labels):\n",
    "                x_grid_size = scale_labels.shape[1]\n",
    "                y_grid_size = scale_labels.shape[2]\n",
    "                num_anchors = scale_labels.shape[3]\n",
    "\n",
    "                for row in range(x_grid_size):\n",
    "                    for col in range(y_grid_size):\n",
    "                        for anchor_idx in range(num_anchors):\n",
    "                            objectness = scale_labels[batch_idx, row, col, anchor_idx, 0]\n",
    "                            \n",
    "                            if objectness > 0:\n",
    "                                x_center, y_center, width, height = scale_labels[batch_idx, row, col, anchor_idx, 1:5]\n",
    "\n",
    "                                title += f\"s_{scale_idx}: g:{row+1}x{col+1} | c: {x_center:.2f} {y_center:.2f}\\n\"\n",
    "\n",
    "                                class_probs = scale_labels[batch_idx, row, col, anchor_idx, 5:]\n",
    "\n",
    "                                # Convert grid cell coordinates to image coordinates\n",
    "                                x_center_image = (row + x_center) * image.shape[1] / x_grid_size\n",
    "                                y_center_image = (col + y_center) * image.shape[0] / y_grid_size\n",
    "                                width_image = width * image.shape[1]\n",
    "                                height_image = height * image.shape[0]\n",
    "\n",
    "                                # Calculate bounding box coordinates\n",
    "                                x_min = int(x_center_image - width_image / 2)\n",
    "                                y_min = int(y_center_image - height_image / 2)\n",
    "                                x_max = int(x_center_image + width_image / 2)\n",
    "                                y_max = int(y_center_image + height_image / 2)\n",
    "\n",
    "                                # Draw bounding box on the image\n",
    "                                class_id = np.argmax(class_probs)\n",
    "                                class_name = class_mapping[class_id] if class_mapping else str(class_id)\n",
    "                                label = f\"scale:{scale_idx} {class_name} ({objectness:.2f})\"\n",
    "                                color = (0, 255, 0)  # Green for the bounding box color\n",
    "                                thickness = 2\n",
    "\n",
    "                                cv2.rectangle(image_with_boxes, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "                                cv2.putText(image_with_boxes, label, (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
    "\n",
    "                                \"\"\"\n",
    "                                # Draw grid lines\n",
    "                                grid_color = (0, 0, 255)  # Red for grid lines\n",
    "                                grid_thickness = 1\n",
    "                                cell_width = image.shape[1] // y_grid_size\n",
    "                                cell_height = image.shape[0] // x_grid_size\n",
    "\n",
    "                                # Draw vertical grid lines\n",
    "                                for i in range(1, y_grid_size):\n",
    "                                    x = i * cell_width\n",
    "                                    cv2.line(image_with_boxes, (x, 0), (x, image.shape[0]), grid_color, grid_thickness)\n",
    "\n",
    "                                # Draw horizontal grid lines\n",
    "                                for i in range(1, x_grid_size):\n",
    "                                    y = i * cell_height\n",
    "                                    cv2.line(image_with_boxes, (0, y), (image.shape[1], y), grid_color, grid_thickness)\n",
    "                                \"\"\"\n",
    "                                    \n",
    "                                # Highlight the grid cell by drawing a rectangle\n",
    "                                grid_color = (255, 0, 0)  # Red for grid lines\n",
    "                                grid_thickness = 1\n",
    "                                cell_width = image.shape[1] // y_grid_size\n",
    "                                cell_height = image.shape[0] // x_grid_size\n",
    "                                \n",
    "                                x_min = row * cell_width\n",
    "                                y_min = col * cell_height\n",
    "                                x_max = (row + 1) * cell_width\n",
    "                                y_max = (col + 1) * cell_height\n",
    "                                \n",
    "                                cv2.rectangle(image_with_boxes, (x_min, y_min), (x_max, y_max), grid_color, grid_thickness)\n",
    "\n",
    "                                # Draw dots at x_center and y_center\n",
    "                                dot_radius = 2\n",
    "                                dot_color = (255, 0, 0)  # Blue for dots\n",
    "                                x_center_pixel = int(x_center_image)\n",
    "                                y_center_pixel = int(y_center_image)\n",
    "\n",
    "                                cv2.circle(image_with_boxes, (x_center_pixel, y_center_pixel), dot_radius, dot_color, -1)\n",
    "\n",
    "            plt.imshow(image_with_boxes)\n",
    "            plt.title(title)\n",
    "            plt.axis(\"off\")                        \n",
    "    \n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "display_batch(data_generator.dataset, class_mapping=class_map, figsize=(20,24))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
