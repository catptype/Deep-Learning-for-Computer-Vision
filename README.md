# Self-Study in Deep Learning with TensorFlow and Keras
A personal repository documenting my deep learning self-study journey with practical implementations using TensorFlow and Keras.

## Table of Contents
- [Introduction](#introduction)
- [Models](#models)
- [Extension Modules](#extension-modules)
- [Evaluation Modules](#evaluation-modules)

## Introduction
This repository serves as a digital archive of my journey into deep learning. My primary is focusing on practical implementations using TensorFlow and Keras. Through this self-study, I aim to gain a deeper understanding of deep learning concepts and techniques.

## Models (sorted by year)
This section provides an overview of the deep learning models that I have explored. Each model is linked to its respective paper for reference.
- [VGG](https://arxiv.org/abs/1409.1556)
- [GoogLeNet (Inception v1)](https://arxiv.org/abs/1409.4842)
- [Residual Network (ResNet)](https://arxiv.org/abs/1512.03385)
- [SqueezeNet](https://arxiv.org/abs/1602.07360)
- [DenseNet](https://arxiv.org/abs/1608.06993)
- [ResNeXt](https://arxiv.org/abs/1611.05431)
- [YOLOv3](https://arxiv.org/abs/1804.02767)
- [Res2Net](https://arxiv.org/abs/1904.01169)
- [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)
- [Compact Transformers: CVT & CCT](https://arxiv.org/abs/2104.05704)

## Extension modules
In addition to the core models, I have explored various extension modules that enhance model performance and capabilities. These modules are essential for creating state-of-the-art deep learning architectures.
- [Squeeze-and-Excitation Networks (SE block)](https://arxiv.org/abs/1709.01507)
- [Convolutional Block Attention Module (CBAM)](https://arxiv.org/abs/1807.06521) 

## Evaluation modules
- [Gradient-weighted Class Activation Mapping (Gran-CAM)](https://arxiv.org/abs/1610.02391)